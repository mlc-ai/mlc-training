{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models Using Relax Training APIs\n",
    "\n",
    "There has been increased interest from the community in using TVM for training. Relax, the next generation graph level IR of TVM, also faces the demand of training model. \n",
    "\n",
    "We built a training workflow on Relax, including:\n",
    "- an **automatic differentiation tool** based on source code transformation\n",
    "- an **optimizer abstraction** and **common optimizers**\n",
    "- a **loss function abstraction** and **common loss functions**\n",
    "- **Trainer API** that integrates them together, and is easy to use\n",
    "\n",
    "The training APIs can serve many needs. You can:\n",
    "- train a model from scratch. You can use the compilation advantages of TVM to speed up the training process.\n",
    "- fine-tune a model on device based on TVM.\n",
    "- deploy the process of training models to various devices that TVM supports, such as FPGA and Raspberry PI.\n",
    "\n",
    "In this tutorial, we will use the training APIs to train a model from scratch using both high-level Trainer API and low-level AD, optimizer, and loss APIs. Finally, we will walk through the source code of the automatic differentiation system to demonstrate how it works.\n",
    "\n",
    "We will use the Fashion MNIST dataset to train an MLP model. This process also applies to most models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "First of all, we will import necessary dependencies and load the DataSet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tvm\n",
    "from tvm.relax.block_builder import BlockBuilder\n",
    "from tvm.relax.training.loss import CrossEntropyLoss\n",
    "from tvm.relax.training.setup_trainer import SetupTrainer\n",
    "from tvm.relax.training.trainer import Trainer\n",
    "from tvm.runtime.container import tuple_object\n",
    "import tvm.script\n",
    "from tvm import relax\n",
    "from tvm.script.parser import ir as I, relax as R, tir as T\n",
    "from tvm.relax.transform import LegalizeOps\n",
    "from tvm.relax.training.optimizer import Adam, SGD\n",
    "\n",
    "batch_size = 64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a model on the [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset. The following code downloads and prepares the data from torchvision.\n",
    "\n",
    "Note that we are using torch only for loading data. The data loaded from PyTorch Dataloader will be transformed into NumPy arrays in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.transforms as Tr\n",
    "import torch.nn.functional as Func\n",
    "\n",
    "train_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=Tr.Compose([Tr.ToTensor(), Tr.Lambda(torch.flatten)]),\n",
    "    target_transform=lambda x:Func.one_hot(torch.tensor(x), 10).float()\n",
    ")\n",
    "test_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=Tr.Compose([Tr.ToTensor(), Tr.Lambda(torch.flatten)]),\n",
    "    target_transform=lambda x:Func.one_hot(torch.tensor(x), 10).float()\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take an example from the dataloader. Every instance in the Fashion MNIST dataset is a $28 \\times 28$ grayscale image and belong to one of the ten clothing classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: Pullover\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img, label = next(iter(train_loader))\n",
    "img = img[0].reshape(1, 28, 28).numpy()\n",
    "plt.figure()\n",
    "plt.imshow(img[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "print(\"Class:\", class_names[label.argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "We will use a 3-layer perceptron to classify images. Firstly we should define the backbone of the perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class Backbone:\n",
    "    @R.function\n",
    "    def predict(\n",
    "        w0: R.Tensor((784, 128), \"float32\"),\n",
    "        b0: R.Tensor((128,), \"float32\"),\n",
    "        w1: R.Tensor((128, 128), \"float32\"),\n",
    "        b1: R.Tensor((128,), \"float32\"),\n",
    "        w2: R.Tensor((128, 10), \"float32\"),\n",
    "        b2: R.Tensor((10,), \"float32\"),\n",
    "        x: R.Tensor((batch_size, 784), \"float32\"),\n",
    "    ) -> R.Tensor((batch_size, 10), \"float32\"):\n",
    "        with R.dataflow():\n",
    "            lv0 = R.matmul(x, w0)\n",
    "            lv1 = R.add(lv0, b0)\n",
    "            lv2 = R.nn.relu(lv1)\n",
    "            lv3 = R.matmul(lv2, w1)\n",
    "            lv4 = R.add(lv3, b1)\n",
    "            lv5 = R.nn.relu(lv4)\n",
    "            lv6 = R.matmul(lv5, w2)\n",
    "            out = R.add(lv6, b2)\n",
    "            R.output(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backbones should satisfy several requirements. Including:\n",
    "- The name of the forward function is \"predict\";\n",
    "- The first parameters of the forward function should be the parameters of the model, and all parameters are of the same float dtype;\n",
    "- The return value of the function is the **output**, not the **loss**\n",
    "- The forward function should contain only one dataflow block, and can only contain these elements:\n",
    "    - Relax operator calls (now we support a subset of Relax operators)\n",
    "    - Assignments\n",
    "    - Constants\n",
    "    - Tuple construction and indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Use Trainer APIs\n",
    "\n",
    "### Structure of a Trainer\n",
    "\n",
    "The easier way to train the given model is using Trainer APIs. Trainer APIs provides the essential interfaces for updating the parameters and inference.\n",
    "\n",
    "To construct a trainer, we should construct an optimizer and a loss function first. We just need to specify **hyperparameters**, such as learning rate, reduction methods, to construct them. Parameters are not needed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CrossEntropyLoss(reduction=\"sum\")\n",
    "opt = SGD(0.01, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, we should construct a `SetupTrainer`. This is the helper class for `Trainer`. It is essentially a pass, which transforms the backbone module to a complete, legalized trainer module.\n",
    "\n",
    "The transformed module will contains the following functions:\n",
    "- `predict`: Predicts the result. It is provided in the input module.\n",
    "- `loss`: Calculates the specified loss between the prediction results and the ground truth.\n",
    "- `loss_adjoint`: Calculates the loss and the adjoints of parameters.\n",
    "- `update_params`: Takes the parameters, the adjoints of parameters and the optimizer state as\n",
    "the inputs and returns updated parameters and new optimizer state. It contains a func attr named\n",
    "`optim_state` which is the initial state of the specified optimizer.\n",
    "\n",
    "`SetupTrainer` requires the loss, the optimizer and the `struct_info` of the output and the label to construct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_sinfo = relax.TensorStructInfo((batch_size, 10), \"float32\")\n",
    "label_sinfo = relax.TensorStructInfo((batch_size, 10), \"float32\")\n",
    "\n",
    "setup_trainer = SetupTrainer(loss, opt, [out_sinfo, label_sinfo])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last step, let's introduce `Trainer`. `Trainer` is a runtime facility. It uses SetupTrainer to setup the backbone and then builds and runs the module. It also maintains the runtime values of parameters internally.\n",
    "\n",
    "To construct a Trainer, we need to specify the backbone, the number of parameters $n$, and the `SetupTrainer` instance. The first $n$ parameters of the backbone will be regarded as the parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(Backbone, 6, setup_trainer)\n",
    "# build the IRModule in the trainer\n",
    "trainer.build(target=\"llvm\", device=tvm.cpu(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process\n",
    "\n",
    "Now that the Trainer has been built, we can perform the common training workflow on top of it. We will randomly initialize the parameters, and train the model for 5 epochs.\n",
    "\n",
    "`Trainer` provides a `xaiver_uniform_init_params` method to initialize all parameters using the Xaiver uniform initialization. If you want to initialize parameters using the given NDArrays, you could call\n",
    "- `trainer.load_params(extern_param_dict: Dict[str, Union[np.ndarray, NDArray]])`\n",
    "- `trainer.export_params() -> Dict[str, NDArray]`\n",
    "\n",
    "to set and export parameters on your own.\n",
    "\n",
    "We will use `update_params` to update the parameters. Internally, this API executes these things:\n",
    "- Doing the forward propagation to obtain output and the loss\n",
    "- Find the gradient w.r.t. the parameters\n",
    "- Using the optimizer to update parameters according to their gradients\n",
    "- Returning the loss\n",
    "\n",
    "The `predict` method is designed for inference. It would receive a batch of features, and returns the prediction, a.k.a. the output of the backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.37\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.10\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.69\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.64\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.69\n",
      "Train Epoch: 0 Accuracy on test dataset: 77.24%\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.67\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.55\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.54\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.63\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.58\n",
      "Train Epoch: 1 Accuracy on test dataset: 80.29%\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.75\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.50\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.70\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.50\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.51\n",
      "Train Epoch: 2 Accuracy on test dataset: 81.16%\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.69\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.50\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.42\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.47\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.42\n",
      "Train Epoch: 3 Accuracy on test dataset: 81.85%\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.44\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.44\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.47\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.46\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.36\n",
      "Train Epoch: 4 Accuracy on test dataset: 82.16%\n"
     ]
    }
   ],
   "source": [
    "trainer.xaiver_uniform_init_params()\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "log_interval = 200\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        loss = trainer.update_params(data.numpy(), target.numpy())\n",
    "\n",
    "        if batch_idx % log_interval == 0 or batch_idx == len(train_loader):\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \"\n",
    "                f\"({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.numpy():.2f}\")\n",
    "\n",
    "    total, correct = 0, 0\n",
    "    for data, target in test_loader:\n",
    "        predict = trainer.predict(data.numpy()) # batch_size * 10\n",
    "        total += len(data)\n",
    "        correct += np.sum(predict.numpy().argmax(1) == target.numpy().argmax(1))\n",
    "\n",
    "    print(f\"Train Epoch: {epoch} Accuracy on test dataset: {100.0 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we have separate `Trainer` and `SetupTrainer`?\n",
    "\n",
    "The reason is the separation of compile time and runtime. All previous components, except `Trainer`, executes in the compile time. After these compile time procedures, we will get a complete `IRModule` that contains all the computation logic, and can be deployed anywhere. In contrast, `Trainer` will receive a `IRModule`, and performs essential parameter updates for it.\n",
    "\n",
    "In many cases, we would use TVM to build an `IRModule` in one device and deploy it in other places. So it is important that we separate the compile time procedure and runtime procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Use Low Level Training APIs\n",
    "\n",
    "We can also use the low level training APIs to build and run the IRModule. These APIs include:\n",
    "- Loss function library\n",
    "- Optimizer library\n",
    "- Automatic Differentiation pass\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "We provides a lot of loss functions in the module `tvm.relax.training.loss`, including `CrossEntropyLoss`, `L1Loss`, `MSELoss`, etc. You can also inherit the `tvm.relax.training.loss.Loss` class to define your own loss functions.\n",
    "\n",
    "The loss class can be constructed only with hyperparameters. On the other hand, its `__call__()` method will receive the struct_info of the output and the label, and generates the corresponding Relax loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@R.function\n",
      "def main(predictions: R.Tensor((64, 10), dtype=\"float32\"), targets: R.Tensor((64, 10), dtype=\"float32\")) -> R.Tensor((), dtype=\"float32\"):\n",
      "    # block 0\n",
      "    with R.dataflow():\n",
      "        lv: R.Tensor((64, 10), dtype=\"float32\") = R.nn.log_softmax(predictions, axis=-1)\n",
      "        gv: R.Tensor((), dtype=\"float32\") = R.nn.cross_entropy_with_logits(lv, targets)\n",
      "        R.output(gv)\n",
      "    return gv\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "func = CrossEntropyLoss(reduction=\"sum\")(out_sinfo, label_sinfo)\n",
    "print(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ought to merge the backbone function and the loss function together because of the demands of the automatic differentiation pass. We provide the `relax.training.utils.append_loss` tool to merge the backbone and the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #AA22FF\">@tvm</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>script<span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">predict</span>(\n",
       "        w0: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        b0: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        w1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        b1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        w2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        b2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "    ) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            lv0: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(x, w0, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;&quot;</span>)\n",
       "            lv1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv0, b0)\n",
       "            lv2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu(lv1)\n",
       "            lv3: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(lv2, w1, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;&quot;</span>)\n",
       "            lv4: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv3, b1)\n",
       "            lv5: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu(lv4)\n",
       "            lv6: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(lv5, w2, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;&quot;</span>)\n",
       "            out: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv6, b2)\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(out)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> out\n",
       "\n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">loss</span>(\n",
       "        w0: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        b0: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        w1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        b1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        w2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        b2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        targets: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "    ) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            lv0: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(x, w0, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;&quot;</span>)\n",
       "            lv1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv0, b0)\n",
       "            lv2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu(lv1)\n",
       "            lv3: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(lv2, w1, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;&quot;</span>)\n",
       "            lv4: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv3, b1)\n",
       "            lv5: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu(lv4)\n",
       "            lv6: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(lv5, w2, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;&quot;</span>)\n",
       "            out: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv6, b2)\n",
       "            lv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>log_softmax(out, axis<span style=\"color: #AA22FF; font-weight: bold\">=-</span><span style=\"color: #008000\">1</span>)\n",
       "            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>cross_entropy_with_logits(\n",
       "                lv, targets\n",
       "            )\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Backbone[\"loss\"] = relax.training.utils.append_loss(Backbone[\"predict\"], func)\n",
    "Backbone.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Pass\n",
    "\n",
    "To optimize the parameters, we need to find the gradients w.r.t. parameters. We provides a automatic differentiation pass `relax.transform.Gradient` to find the gradient.\n",
    "\n",
    "The AD system is the core of our training workflow. It is basically based on the source code transformation method. Now the AD systems have several restrictions on the input function.\n",
    "\n",
    "- \n",
    "- \n",
    "\n",
    "`Gradient` receives the global var of the given function, the variables to find gradient w.r.t., and the input IRModule. It will return the transformed IRModule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #AA22FF\">@tvm</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>script<span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">predict</span>(\n",
       "        w0: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        b0: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        w1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        b1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        w2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        b2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "    ) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            lv0: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(x, w0, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;&quot;</span>)\n",
       "            lv1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv0, b0)\n",
       "            lv2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu(lv1)\n",
       "            lv3: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(lv2, w1, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;&quot;</span>)\n",
       "            lv4: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv3, b1)\n",
       "            lv5: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu(lv4)\n",
       "            lv6: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(lv5, w2, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;&quot;</span>)\n",
       "            out: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv6, b2)\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(out)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> out\n",
       "\n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">loss</span>(\n",
       "        w0: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        b0: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        w1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        b1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        w2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        b2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        targets: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "    ) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            lv0: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(x, w0, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;&quot;</span>)\n",
       "            lv1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv0, b0)\n",
       "            lv2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu(lv1)\n",
       "            lv3: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(lv2, w1, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;&quot;</span>)\n",
       "            lv4: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv3, b1)\n",
       "            lv5: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu(lv4)\n",
       "            lv6: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(lv5, w2, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;&quot;</span>)\n",
       "            out: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv6, b2)\n",
       "            lv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>log_softmax(out, axis<span style=\"color: #AA22FF; font-weight: bold\">=-</span><span style=\"color: #008000\">1</span>)\n",
       "            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>cross_entropy_with_logits(\n",
       "                lv, targets\n",
       "            )\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "\n",
       "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">loss_adjoint</span>(\n",
       "        w0: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        b0: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        w1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        b1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        w2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        b2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        targets: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "    ) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tuple(\n",
       "        R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tuple(\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>),\n",
       "        ),\n",
       "    ):\n",
       "        <span style=\"color: #007979; font-style: italic\"># block 0</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
       "            lv0: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(x, w0, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;&quot;</span>)\n",
       "            lv1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv0, b0)\n",
       "            lv2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu(lv1)\n",
       "            lv3: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(lv2, w1, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;&quot;</span>)\n",
       "            lv4: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv3, b1)\n",
       "            lv5: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu(lv4)\n",
       "            lv6: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(lv5, w2, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;&quot;</span>)\n",
       "            out: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv6, b2)\n",
       "            lv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>log_softmax(out, axis<span style=\"color: #AA22FF; font-weight: bold\">=-</span><span style=\"color: #008000\">1</span>)\n",
       "            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>cross_entropy_with_logits(\n",
       "                lv, targets\n",
       "            )\n",
       "            gv_adjoint: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>ones((), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
       "            lv7: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>divide(\n",
       "                gv_adjoint, R<span style=\"color: #AA22FF; font-weight: bold\">.</span>const(<span style=\"color: #008000\">64</span>, <span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
       "            )\n",
       "            lv11: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>multiply(lv7, targets)\n",
       "            lv_adjoint: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>negative(lv11)\n",
       "            lv21: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">1</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>sum(\n",
       "                lv_adjoint, axis<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #AA22FF; font-weight: bold\">-</span><span style=\"color: #008000\">1</span>], keepdims<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>\n",
       "            )\n",
       "            lv31: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>exp(lv)\n",
       "            lv41: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>multiply(lv21, lv31)\n",
       "            out_adjoint: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>subtract(\n",
       "                lv_adjoint, lv41\n",
       "            )\n",
       "            lv6_adjoint: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> out_adjoint\n",
       "            lv51: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>permute_dims(w2, axes<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">0</span>])\n",
       "            lv61: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(\n",
       "                lv6_adjoint, lv51, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;&quot;</span>\n",
       "            )\n",
       "            lv5_adjoint: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>collapse_sum_to(\n",
       "                lv61, (<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>)\n",
       "            )\n",
       "            lv71: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>zeros(\n",
       "                (<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>\n",
       "            )\n",
       "            lv8: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;bool&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>less(lv4, lv71)\n",
       "            lv9: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>ones(\n",
       "                (<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>\n",
       "            )\n",
       "            lv10: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>multiply(lv9, lv5_adjoint)\n",
       "            lv4_adjoint: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>where(lv8, lv71, lv10)\n",
       "            lv3_adjoint: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv4_adjoint\n",
       "            lv111: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>permute_dims(\n",
       "                w1, axes<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">0</span>]\n",
       "            )\n",
       "            lv2_adjoint: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(\n",
       "                lv3_adjoint, lv111, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;&quot;</span>\n",
       "            )\n",
       "            lv12: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>zeros(\n",
       "                (<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>\n",
       "            )\n",
       "            lv13: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;bool&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>less(lv1, lv12)\n",
       "            lv14: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>ones(\n",
       "                (<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>\n",
       "            )\n",
       "            lv15: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>multiply(lv14, lv2_adjoint)\n",
       "            lv1_adjoint: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>where(\n",
       "                lv13, lv12, lv15\n",
       "            )\n",
       "            lv0_adjoint: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv1_adjoint\n",
       "            lv16: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>permute_dims(x, axes<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">0</span>])\n",
       "            lv17: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(\n",
       "                lv16, lv0_adjoint, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;&quot;</span>\n",
       "            )\n",
       "            w0_adjoint: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>collapse_sum_to(\n",
       "                lv17, (<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">128</span>)\n",
       "            )\n",
       "            b0_adjoint: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>collapse_sum_to(\n",
       "                lv1_adjoint, (<span style=\"color: #008000\">128</span>,)\n",
       "            )\n",
       "            lv18: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>permute_dims(\n",
       "                lv2, axes<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">0</span>]\n",
       "            )\n",
       "            lv19: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(\n",
       "                lv18, lv3_adjoint, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;&quot;</span>\n",
       "            )\n",
       "            w1_adjoint: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>collapse_sum_to(\n",
       "                lv19, (<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>)\n",
       "            )\n",
       "            b1_adjoint: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>collapse_sum_to(\n",
       "                lv4_adjoint, (<span style=\"color: #008000\">128</span>,)\n",
       "            )\n",
       "            lv20: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>permute_dims(\n",
       "                lv5, axes<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">0</span>]\n",
       "            )\n",
       "            lv211: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(\n",
       "                lv20, lv6_adjoint, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;&quot;</span>\n",
       "            )\n",
       "            w2_adjoint: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>collapse_sum_to(\n",
       "                lv211, (<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">10</span>)\n",
       "            )\n",
       "            b2_adjoint: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>collapse_sum_to(\n",
       "                out_adjoint, (<span style=\"color: #008000\">10</span>,)\n",
       "            )\n",
       "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(\n",
       "                gv,\n",
       "                w0_adjoint,\n",
       "                b0_adjoint,\n",
       "                w1_adjoint,\n",
       "                b1_adjoint,\n",
       "                w2_adjoint,\n",
       "                b2_adjoint,\n",
       "            )\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> (\n",
       "            gv,\n",
       "            (w0_adjoint, b0_adjoint, w1_adjoint, b1_adjoint, w2_adjoint, b2_adjoint),\n",
       "        )\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = Backbone[\"loss\"].params[:6]\n",
    "\n",
    "Backbone = relax.transform.Gradient(\n",
    "    Backbone.get_global_var(\"loss\"),\n",
    "    require_grads=params\n",
    ")(Backbone)\n",
    "Backbone.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "We also provides a bunch of common optimizers in `relax.training.optimizer`, including SGD, SGD with momentum, and Adam. You can also extend the `relax.training.optimizer.Optimizer` to create your own optimizer.\n",
    "\n",
    "\n",
    "The optimizer API can also be constructed only with hyperparameters. An optimizer should be initialized with `init()`, which receives one or a list of Relax Vars to optimize, and initialize the state of the optimizer.\n",
    "\n",
    "After the optimizer is initialized, we can call `get_function()` to get the corresponding Relax optimizer function, or assign it to a IRModule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@R.function\n",
      "def main(params: R.Tuple(R.Tensor((784, 128), dtype=\"float32\"), R.Tensor((128,), dtype=\"float32\"), R.Tensor((128, 128), dtype=\"float32\"), R.Tensor((128,), dtype=\"float32\"), R.Tensor((128, 10), dtype=\"float32\"), R.Tensor((10,), dtype=\"float32\")), gradients: R.Tuple(R.Tensor((784, 128), dtype=\"float32\"), R.Tensor((128,), dtype=\"float32\"), R.Tensor((128, 128), dtype=\"float32\"), R.Tensor((128,), dtype=\"float32\"), R.Tensor((128, 10), dtype=\"float32\"), R.Tensor((10,), dtype=\"float32\")), optim_states: R.Tuple(R.Tensor((), dtype=\"int64\"))) -> R.Tuple(R.Tuple(R.Tensor((784, 128), dtype=\"float32\"), R.Tensor((128,), dtype=\"float32\"), R.Tensor((128, 128), dtype=\"float32\"), R.Tensor((128,), dtype=\"float32\"), R.Tensor((128, 10), dtype=\"float32\"), R.Tensor((10,), dtype=\"float32\")), R.Tuple(R.Tensor((), dtype=\"int64\"))):\n",
      "    # block 0\n",
      "    with R.dataflow():\n",
      "        num_steps: R.Tensor((), dtype=\"int64\") = optim_states[0]\n",
      "        num_steps_new: R.Tensor((), dtype=\"int64\") = R.add(num_steps, R.const(1, \"int64\"))\n",
      "        w0: R.Tensor((784, 128), dtype=\"float32\") = params[0]\n",
      "        w0_grad: R.Tensor((784, 128), dtype=\"float32\") = gradients[0]\n",
      "        lv: R.Tensor((784, 128), dtype=\"float32\") = R.multiply(R.const(0.1, \"float32\"), w0_grad)\n",
      "        w0_new: R.Tensor((784, 128), dtype=\"float32\") = R.subtract(w0, lv)\n",
      "        b0: R.Tensor((128,), dtype=\"float32\") = params[1]\n",
      "        b0_grad: R.Tensor((128,), dtype=\"float32\") = gradients[1]\n",
      "        lv1: R.Tensor((128,), dtype=\"float32\") = R.multiply(R.const(0.1, \"float32\"), b0_grad)\n",
      "        b0_new: R.Tensor((128,), dtype=\"float32\") = R.subtract(b0, lv1)\n",
      "        w1: R.Tensor((128, 128), dtype=\"float32\") = params[2]\n",
      "        w1_grad: R.Tensor((128, 128), dtype=\"float32\") = gradients[2]\n",
      "        lv2: R.Tensor((128, 128), dtype=\"float32\") = R.multiply(R.const(0.1, \"float32\"), w1_grad)\n",
      "        w1_new: R.Tensor((128, 128), dtype=\"float32\") = R.subtract(w1, lv2)\n",
      "        b1: R.Tensor((128,), dtype=\"float32\") = params[3]\n",
      "        b1_grad: R.Tensor((128,), dtype=\"float32\") = gradients[3]\n",
      "        lv3: R.Tensor((128,), dtype=\"float32\") = R.multiply(R.const(0.1, \"float32\"), b1_grad)\n",
      "        b1_new: R.Tensor((128,), dtype=\"float32\") = R.subtract(b1, lv3)\n",
      "        w2: R.Tensor((128, 10), dtype=\"float32\") = params[4]\n",
      "        w2_grad: R.Tensor((128, 10), dtype=\"float32\") = gradients[4]\n",
      "        lv4: R.Tensor((128, 10), dtype=\"float32\") = R.multiply(R.const(0.1, \"float32\"), w2_grad)\n",
      "        w2_new: R.Tensor((128, 10), dtype=\"float32\") = R.subtract(w2, lv4)\n",
      "        b2: R.Tensor((10,), dtype=\"float32\") = params[5]\n",
      "        b2_grad: R.Tensor((10,), dtype=\"float32\") = gradients[5]\n",
      "        lv5: R.Tensor((10,), dtype=\"float32\") = R.multiply(R.const(0.1, \"float32\"), b2_grad)\n",
      "        b2_new: R.Tensor((10,), dtype=\"float32\") = R.subtract(b2, lv5)\n",
      "        params_new: R.Tuple(R.Tensor((784, 128), dtype=\"float32\"), R.Tensor((128,), dtype=\"float32\"), R.Tensor((128, 128), dtype=\"float32\"), R.Tensor((128,), dtype=\"float32\"), R.Tensor((128, 10), dtype=\"float32\"), R.Tensor((10,), dtype=\"float32\")) = (w0_new, b0_new, w1_new, b1_new, w2_new, b2_new)\n",
      "        optim_states_new: R.Tuple(R.Tensor((), dtype=\"int64\")) = (num_steps_new,)\n",
      "        R.output(params_new, optim_states_new)\n",
      "    return (params_new, optim_states_new)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "opt = relax.optimizer.SGD(0.1).init(params)\n",
    "Backbone[\"SGD\"] = opt.get_function()\n",
    "print(Backbone[\"SGD\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process\n",
    "\n",
    "Finally we can train our model using the constructed `IRModule`. We should legalize and build the IRModule, and then construct the necessary inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and legalize module\n",
    "lowered_mod = LegalizeOps()(Backbone)\n",
    "ex = relax.vm.build(lowered_mod, target=\"llvm\")\n",
    "vm = relax.VirtualMachine(ex, tvm.cpu())\n",
    "\n",
    "\n",
    "def _get_shape_as_int_list(var):\n",
    "    return [int(val) for val in var.struct_info.shape]\n",
    "\n",
    "params_list = [tvm.nd.array(np.ones(_get_shape_as_int_list(i), \"float32\")) for i in params]\n",
    "param_input_tuple = tuple_object(params_list)\n",
    "\n",
    "x_input, y_input = next(iter(train_loader))\n",
    "x_input = tvm.nd.array(x_input)\n",
    "y_input = tvm.nd.array(y_input)\n",
    "\n",
    "# The input should be (*param_input_tuple, x_input, y_input)\n",
    "# At the runtime of TVM, arguments should be TVM NDArray or TVM runtime ADT objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only train our model for one step in this demonstration. However, training more steps is quite similar.\n",
    "\n",
    "The adjoint function, which is returned by the AD Pass, receives the inputs of the backbone and the label, and returns the loss and a tuple of the gradients of the parameters.\n",
    "\n",
    "The optimizer function, which is constructed by the optimizer class, receives a tuple of parameters, a tuple of the gradient of parameters, and a tuple of optimizer state. It would return a tuple of updated parameters, and a tuple of updated optimizer states.\n",
    "\n",
    "Here the optimizer state would be gained by `opt.state`, and would store some information important in the optimization process, such as the number of steps, the momentum, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward and find the gradient\n",
    "loss, param_grad_tuple = vm[\"loss_adjoint\"](*param_input_tuple, x_input, y_input)\n",
    "# update parameters\n",
    "param_input_tuple, opt.state = vm[\"SGD\"](param_input_tuple, param_grad_tuple, opt.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the result of the computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDArray.numpy of <tvm.nd.NDArray shape=(), cpu(0)>\n",
      "array(2.3025835, dtype=float32)>\n",
      "6 6\n",
      "[[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " ...\n",
      " [-6.1942089e-08 -6.1942089e-08 -6.1942089e-08 ... -6.1942089e-08\n",
      "  -6.1942089e-08 -6.1942089e-08]\n",
      " [-2.2906884e-08 -2.2906884e-08 -2.2906884e-08 ... -2.2906884e-08\n",
      "  -2.2906884e-08 -2.2906884e-08]\n",
      " [-1.0518467e-08 -1.0518467e-08 -1.0518467e-08 ... -1.0518467e-08\n",
      "  -1.0518467e-08 -1.0518467e-08]]\n"
     ]
    }
   ],
   "source": [
    "print(loss.numpy)\n",
    "print(len(param_input_tuple), len(param_grad_tuple))\n",
    "print(param_input_tuple[0])\n",
    "print(param_grad_tuple[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
